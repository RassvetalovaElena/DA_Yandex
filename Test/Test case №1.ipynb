{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5dba2cd-1154-4b1d-ac52-f348658f7df6",
   "metadata": {},
   "source": [
    "# Test case №1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b2271-6d72-4242-9caa-64258320f082",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d264965-7581-44f8-b464-7720c8959f89",
   "metadata": {},
   "source": [
    "***We extracted data from GCP. The dataset contains 4 million rows, but we were only able to extract them in small portions, about 40,000 rows in CSV format. In total, we ended up with 110 files. The files are named as follows:***\n",
    "\n",
    "data_000000000001.csv\n",
    "data_000000000002.csv\n",
    "…..\n",
    "data_000000000010.csv\n",
    "data_000000000011.csv\n",
    "……\n",
    "data_000000000100.csv\n",
    "data_000000000101.csv\n",
    "….\n",
    "data_000000000110.csv\n",
    "\n",
    "***Please suggest a method for uploading all these 110 files using Jupyter Notebook (or another) and merging them into a single dataset for further processing.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52573257-fc06-48ad-8947-93a6685c2807",
   "metadata": {},
   "source": [
    "The most logical step would be to first merge the data into one file. This could be done using a Python command-line utility called ***csvkit****. It can be installed as a Python module using the following command: ***pip install csvkit***. Alternatively, you can use a script in Jupyter Notebook for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168809a-9a58-44a2-89a0-15a9312d062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data = pd.DataFrame()\n",
    "folder_path = 'path/csv/'\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        current_data = pd.read_csv(file_path)\n",
    "        data = data.append(current_data, ignore_index=True)\n",
    "data.to_csv('path/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffcf3c4-fe87-40ec-b742-48fc85a746c4",
   "metadata": {},
   "source": [
    "However, since the size of the merged file will likely be very large and the available RAM may not allow for quick loading and processing of such a file, I suggest three alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcea177-3dca-4358-b6af-b58229b4fc89",
   "metadata": {},
   "source": [
    "## Option 1: Loading the first file, data_000000000001.csv, and analyzing its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b7afb-6e22-4ee6-a8b4-31833bc20cf3",
   "metadata": {},
   "source": [
    "By default, for time efficiency, pandas provides approximate memory usage information for a DataFrame object. However, we are interested in precise details, so we will set the memory_usage parameter to 'deep'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c896f9-fc7f-4b67-874d-464362c95ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data_000000000001.csv',encoding='UTF')\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: \n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 \n",
    "    return \"{:03.2f} MB\".format(usage_mb)\n",
    "print (df.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52c1287-08eb-443f-a454-ce7c977416b6",
   "metadata": {},
   "source": [
    "### Let's analyze the data types in data_000000000001.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ad475a-8fb7-44e1-a03e-0a77e84ebb9d",
   "metadata": {},
   "source": [
    "Once we obtain this information, we will see the data types present in our DataFrame. With this knowledge, we can optimize memory usage by choosing appropriate data types for individual columns during loading. The pandas.read_csv() function has several parameters that allow us to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee262d-93c6-4baa-a00f-659e3986a7c1",
   "metadata": {},
   "source": [
    "The task does not specify the columns present in the dataset. Let's assume they are labeled as a, b, c, and d. For example, if a column contains dates, for the convenience of further calculations and clarity, it is better to set them as the index of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521bc89-7ab8-4dfc-8b94-2f84c1dcce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_000000000001.csv', parse_dates=['b'], encoding='UTF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51232de-8537-46aa-b5d8-9df7a9c53789",
   "metadata": {},
   "source": [
    "Now the dates are read as the index of the dataset, and memory consumption has slightly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ef306-3b25-4478-984e-6f7efba99e22",
   "metadata": {},
   "source": [
    "If there are object types in the data, let's combine the data in columns into categories where it is efficient. To determine efficiency, it's necessary to find the count of unique values in the columns, and if it is less than 50% of the total number of values in the column, combining values into categories will be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8505e10-6c01-490c-801d-9a74272c0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj=df.select_dtypes(include=['object']).copy()\n",
    "df_obj.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799cdf2-762a-4ca2-87bf-fdcf2c47eca6",
   "metadata": {},
   "source": [
    "Columns can be optimized in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9f09a-022d-4b51-8c4f-42e106d90dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in gl_obj.columns:\n",
    "    num_unique_values = len(gl_obj[col].unique())\n",
    "    num_total_values = len(gl_obj[col])\n",
    "    if num_unique_values / num_total_values < 0.5:\n",
    "        converted_obj.loc[:,col] = gl_obj[col].astype('category')        \n",
    "    else:\n",
    "        converted_obj.loc[:,col] = gl_obj[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232137b9-609e-46c7-a81a-b2222ea3f6f7",
   "metadata": {},
   "source": [
    "To understand how much memory is being used, let's introduce a function for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5b2a4-e04a-4286-8a61-97c7d799dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: \n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 \n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137f116-85bd-4034-a39e-297056f42818",
   "metadata": {},
   "source": [
    "And we will check whether the optimization was effective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68aa3b8-4ff9-463d-87d0-d8426ef98cdd",
   "metadata": {},
   "source": [
    "### Let's load all the files into one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa5a8e-3426-41e2-aae8-98bb3e9a046c",
   "metadata": {},
   "source": [
    "Now that we see that the optimization has been beneficial, let's set dataset parameters right at the reading stage to immediately consider the data we are dealing with.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8addae3-37fa-4ff9-801b-dfd08a765be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame()\n",
    "folder_path = '/path/csv/'\n",
    "files = glob.glob(folder_path + '*.csv')\n",
    "for file in files:\n",
    "    data = pd.read_csv(file, parse_dates=['b'], index_col='b',\n",
    "                               dtype={'c': 'category', 'a': 'category', 'd': 'object'}, encoding='UTF')\n",
    "    data = data.append(current_data, ignore_index=True)\n",
    "data.to_csv('/path/data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71013eef-511f-4230-8f4c-d2b933e3b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv',encoding='UTF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c64e88-a03a-4931-8a55-6e7d0028e840",
   "metadata": {},
   "source": [
    "## Option 2: Using the chunksize parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3c1f6-831f-461f-aacb-e7a9017a67ae",
   "metadata": {},
   "source": [
    "If the first option is not successful, you can try using the chunksize parameter for efficient memory usage when working with large CSV files in pandas. This parameter allows processing data in chunks, avoiding loading the entire content of the file into memory at once. This approach optimally utilizes memory since the file is not loaded in its entirety, and the reading process is done line by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe33301-e55a-4596-8da9-5de5af9e7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv('large_file.csv', chunksize=chunk_size)\n",
    "for chunk in chunks:\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990691bd-a784-42a0-9478-556bf724faff",
   "metadata": {},
   "source": [
    "## Option 3: Without using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1743c7e-0db0-48f2-b415-3c5a56704f8b",
   "metadata": {},
   "source": [
    "In this approach, files are also read line by line, and there won't be any memory overload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf294aaa-32fd-4671-a7eb-3b706b736dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/path/csv/'\n",
    "combined_data = []\n",
    "\n",
    "for i in range(1, 111):\n",
    "    file_path = f\"{folder_path}data_{i:012d}.csv\"\n",
    "    \n",
    "    with open(file_path, 'r', encoding='UTF') as file:\n",
    "        if i > 1:\n",
    "            next(file)\n",
    "        \n",
    "        for line in file:\n",
    "            combined_data.append(line.strip())\n",
    "\n",
    "\n",
    "with open('/path/data.csv', 'w', encoding='UTF') as combined_file:\n",
    "    combined_file.write(data[0] + '\\n')\n",
    "    \n",
    "    for line in data[1:]:\n",
    "        combined_file.write(line + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
